getTrainPerf(mod3)
t <- getModelInfo()
m <- list();
for (i in names(t)){
if (t[[i]]$type != "Regression"){
m <- c(m, t[i])
}
}
names(m)[1:5]
getTrainPerf(mod2) #
?predict
??predict
mod2.out <- predict(mod2.out, newdata = df.rest)
mod2.out <- predict(mod2, newdata = df.rest)
mod2.out <- predict(mod2, newdata = df.rest)
mod2.out <- predict(mod2, newdata = df.rest, type = "prob")
cvCtrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)   # see help for other CV
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear",
trControl = cvCtrl)
getTrainPerf(mod2) # S
getTrainPerf(mod2) # around 70% accurate during CV
set.seed(1)
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
Here is an SVM
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
getTrainPerf(mod2) # around 70% accurate during CV
mod2.out <- predict(mod2, newdata = df.rest, type = "prob")
mod2.out <- predict(mod2, newdata = df.rest)
?predict
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
getTrainPerf(mod2) # around 70% accurate during CV
mod2.out <- predict(mod2, newdata = df.rest)
mod2.out <- predict(mod2, newdata = df.rest[,2:31])
confusionMatrix(data = mod2.out, reference = df.rest$rich)
length(mod2.out)
inSubset
inSubset <- createDataPartition(df2$rich, p=0.75, list=FALSE)
df.sub   <- df2[inSubset,]
df.rest  <- df2[-inSubset,]
mod.sub <- train(x= as.matrix(df.sub[,2:31]), y = as.factor(df.sub$rich),
method = "svmLinear")
getTrainPerf(mod.sub)
print(mod.sub)
plot(varImp(mod.sub))                             # all of the predictors!
plot(varImp(mod.sub), top = 10)                   # Just top 10, scaled
plot(varImp(mod.sub, scale = FALSE), top = 10)    # Can have raw importance
coef(mod.sub$finalModel) %>% head()
cvCtrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)   # see help for other CV
mod1 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "lda",
trControl = cvCtrl)
getTrainPerf(mod1) #not great! this is the average cross-validated performance
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
getTrainPerf(mod2) # around 70% accurate during CV
mod2.out <- predict(mod2, newdata = df.rest[,2:31])
confusionMatrix(data = mod2.out, reference = df.rest$rich)
length(mod2.out)
svmGrid <- expand.grid(.sigma = c(1, 0.1, 0.05),
.C = c(1.0, 0.5, 0.1))
# We just have to pass this tuning grid to the train command (w/ CV)
# Tying it all together
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, that sucked. why?
print(mod6) # Inspect the model summary
mod6.out <- predict(mod6, newdata = df.rest[,2:31])
t
confusionMatrix(data = mod6.out, reference = df.rest$rich)
library(caret); library(dplyr)
## Generate data
set.seed(1)
df <- twoClassSim(n = 100,
linearVars = 5, noiseVars = 5,
corrVars = 5, corrValue = 0.5)[,3:21]
df <- df %>%
mutate(rich = factor(Class, labels = c("yes", "no") ) ) %>%
select(rich, Linear1:Corr5, -Class)
names(df)[2:6] <- c("age", "sex", "income", "race", "employment")
names(df)[7:11] <- c("diabetes", "hypertension", "cancer","red", "blue")
names(df)[12:16] <- c("depression", "infection", "green", "yellow", "black")
df <- df %>% select(-Corr3, -Corr4, -Corr5)
df$sex <- ifelse(df$sex < median(df$sex), "0", "1")
df$income <- -1 * df$income
write.csv(df, "class_data.csv")
#glm(rich ~ . , data = df, family = "binomial") %>% summary()
setwd("~/Documents/PhD/PhD_Core/Teaching/yale_ml_workshop")
##############
##############
# Load libraries
library(ggplot2); library(ggplot2); library(dplyr)
## Read in class data
#     Data contains sociodemographics, medical history, and color prefs,
#     for 250 people bunch of people surveyed last week.
#     The outcome of interest here is whether they self-identify as rich
df <- read.csv("class_data.csv", as.is=TRUE)
df$X <- NULL
df$rich <- as.factor(df$rich)
head(df[,1:8])
complete.cases(df) %>% table()
ggplot(data = df) +
geom_boxplot(aes(y = income, x = rich)) + coord_flip()
lr1 <- glm(rich ~ ., family = "binomial", data = df)
summary(lr1)
lr1.out <- fitted(lr1)
ggplot(data = cbind(df, lr1.out)) +
geom_violin(aes(y = lr1.out, x = rich)) + coord_flip() +
geom_boxplot(aes(y = lr1.out, x = rich), width = 0.2 )
lr1.bin.out <- ifelse(lr1.out < 0.5, "no", "yes")
ggplot(data = cbind(df, lr1.out)) +
geom_boxplot(aes(y = lr1.out, x = rich), width = 0.2 ) +
coord_flip()
lr1.bin.out <- ifelse(lr1.out < 0.5, "no", "yes")
confusionMatrix(data = lr1.bin.out, reference = df$rich)
library(ggplot2); library(caret); library(dplyr)
confusionMatrix(data = lr1.bin.out, reference = df$rich)
df2 <- cbind(df, (df[,2:16] + matrix(nrow = 100, ncol = 15, rnorm(15*100))))
names(df2)[17:31] <- paste0(names(df[2:16]), "_2")
lr2 <- glm(rich ~ ., family = "binomial", data = df2)
df.pc <- prcomp(df2[,-1])$x %>% as.data.frame()
df.pc <- df.pc[,1:11]
df.pc$rich <- df2$rich
pc.LR <- glm(rich ~ ., family = "binomial", data = df.pc)
summary(pc.LR)
pc.LR.out <- pc.LR$fitted.values
pc.LR.out <- ifelse(pc.LR.out < 0.5, "no", "yes")
confusionMatrix(data = pc.LR.out, reference = df.pc$rich)
correlations <- sapply(names(df2)[2:31], function(i) cor(as.numeric(df2[,i]), as.numeric(df2$rich)))
summary(correlations)
fs1 <- names(correlations)[(correlations > 0.1) | (correlations < -1.5)]
df.fs1 <- dplyr::select(df2, one_of(c("rich", fs1)))
fs1.lr <- glm(rich ~ ., family = "binomial", data = df.fs1)
confusionMatrix(data = ifelse(fs1.lr$fitted.values < 0.5, "no", "yes"), reference = df.fs1$rich)
set.seed(1)    # this means that we can get the same random split next time
inSubset <- createDataPartition(df2$rich, p=0.75, list=FALSE)
df.sub   <- df2[inSubset,]
df.rest  <- df2[-inSubset,]
mod.sub <- train(x= as.matrix(df.sub[,2:31]), y = as.factor(df.sub$rich),
method = "svmLinear")
getTrainPerf(mod.sub)
print(mod.sub)
plot(varImp(mod.sub))                             # all of the predictors!
plot(varImp(mod.sub), top = 10)                   # Just top 10, scaled
plot(varImp(mod.sub, scale = FALSE), top = 10)    # Can have raw importance
coef(mod.sub$finalModel) %>% head()
cvCtrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)   # see help for other CV
mod1 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "lda",
trControl = cvCtrl)
getTrainPerf(mod1) #not great! this is the average cross-validated performance
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
set.seed(1)
mod1 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "lda",
trControl = cvCtrl)
getTrainPerf(mod1) #not great! this is the average cross-validated performance
set.seed(1)
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did better than LDA
getTrainPerf(mod2) # around 70% accurate during CV
mod2.out <- predict(mod2, newdata = df.rest[,2:31])
confusionMatrix(data = mod2.out, reference = df.rest$rich)
set.seed(1)
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM
mod2.out <- predict(mod2, newdata = df.rest[,2:31])
confusionMatrix(data = mod2.out, reference = df.rest$rich)
svmGrid <- expand.grid(.sigma = c(1, 0.1, 0.05),
.C = c(1.0, 0.5, 0.1))
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
set.seed(4)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
set.seed(5)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
set.seed(6)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
set.seed(7)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
set.seed(11)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, high. why?
print(mod6) # Inspect the model summary
mod6.out <- predict(mod6, newdata = df.rest[,2:31])
confusionMatrix(data = mod6.out, reference = df.rest$rich)
set.seed(12)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
print(mod6) # Inspect the model summary
set.seed(15)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(18)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
svmGrid <- expand.grid(.sigma = c(1, 0.1, 0.05),
.C = c(1.0, 0.5, 0.1, 0.05))
set.seed(18)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(1)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(2)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(3)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(4)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(7)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(8)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
set.seed(123)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
print(mod6) # Inspect the model summary
mod6.out <- predict(mod6, newdata = df.rest[,2:31])
confusionMatrix(data = mod6.out, reference = df.rest$rich)
mod6.out <- predict(mod6, newdata = df.rest[,2:31])
confusionMatrix(data = mod6.out, reference = df.rest$rich)
mod6.out <- predict(mod6, newdata = df.rest[,2:31])
confusionMatrix(data = mod6.out, reference = df.rest$rich)
###########
##############
######### Set working directory to the repository you downloaded
setwd("~/Documents/PhD/PhD_Core/Teaching/yale_ml_workshop")
##############
##############
# Load libraries
library(ggplot2); library(caret); library(dplyr)
## Read in class data
#     Data contains sociodemographics, medical history, and color prefs,
#     for 250 people bunch of people surveyed last week.
#     The outcome of interest here is whether they self-identify as rich
df <- read.csv("class_data.csv", as.is=TRUE)
df$X <- NULL
df$rich <- as.factor(df$rich)
## View data in Rstudio
# View(df)
## View top of data frame in console
head(df[,1:8])
## Any missing data?
complete.cases(df) %>% table()
## Do higher income ppl identify as rich?
ggplot(data = df) +
geom_boxplot(aes(y = income, x = rich)) + coord_flip()
## Typical approach: logistic regression
lr1 <- glm(rich ~ ., family = "binomial", data = df)
summary(lr1)
# In this case, we would probably stop here!
# extract fitted values, see how well they predict feeling rich
lr1.out <- fitted(lr1)
# looks pretty good, considerable separation according to our predicted values
ggplot(data = cbind(df, lr1.out)) +
geom_boxplot(aes(y = lr1.out, x = rich), width = 0.2 ) +
coord_flip()
lr1.bin.out <- ifelse(lr1.out < 0.5, "no", "yes")
confusionMatrix(data = lr1.bin.out, reference = df$rich)
df2 <- cbind(df, (df[,2:16] + matrix(nrow = 100, ncol = 15, rnorm(15*100))))
names(df2)[17:31] <- paste0(names(df[2:16]), "_2")
lr2 <- glm(rich ~ ., family = "binomial", data = df2)
df.pc <- prcomp(df2[,-1])$x %>% as.data.frame()
df.pc <- df.pc[,1:11]
df.pc$rich <- df2$rich
pc.LR <- glm(rich ~ ., family = "binomial", data = df.pc)
summary(pc.LR)
pc.LR.out <- pc.LR$fitted.values
pc.LR.out <- ifelse(pc.LR.out < 0.5, "no", "yes")
confusionMatrix(data = pc.LR.out, reference = df.pc$rich)
correlations <- sapply(names(df2)[2:31], function(i) cor(as.numeric(df2[,i]), as.numeric(df2$rich)))
summary(correlations)
fs1 <- names(correlations)[(correlations > 0.1) | (correlations < -1.5)]
df.fs1 <- dplyr::select(df2, one_of(c("rich", fs1)))
fs1.lr <- glm(rich ~ ., family = "binomial", data = df.fs1)
confusionMatrix(data = ifelse(fs1.lr$fitted.values < 0.5, "no", "yes"), reference = df.fs1$rich)
mySBF <- caretSBF
set.seed(1)    # this means that we can get the same random split next time
inSubset <- createDataPartition(df2$rich, p=0.75, list=FALSE)
df.sub   <- df2[inSubset,]
df.rest  <- df2[-inSubset,]
mod.sub <- train(x= as.matrix(df.sub[,2:31]), y = as.factor(df.sub$rich),
method = "svmLinear")
getTrainPerf(mod.sub)
print(mod.sub)
plot(varImp(mod.sub))                             # all of the predictors!
plot(varImp(mod.sub), top = 10)                   # Just top 10, scaled
plot(varImp(mod.sub, scale = FALSE), top = 10)    # Can have raw importance
coef(mod.sub$finalModel) %>% head()
cvCtrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)   # see help for other CV
set.seed(1)
mod1 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "lda",
trControl = cvCtrl)
getTrainPerf(mod1) #not great! this is the average cross-validated performance
set.seed(1)
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2) # SVM did about the same as LDA
t <- getModelInfo()
m <- list();
for (i in names(t)){
if (t[[i]]$type != "Regression"){
m <- c(m, t[i])
}
}
names(m)[1:5]
mod2.out <- predict(mod2, newdata = df.rest[,2:31])
confusionMatrix(data = mod2.out, reference = df.rest$rich)
svmGrid <- expand.grid(.sigma = c(1, 0.1, 0.05),
.C = c(1.0, 0.5, 0.1, 0.05))
set.seed(123)
mod6 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # seems higher. why?
print(mod6) # Inspect the model summary
mod6.out <- predict(mod6, newdata = df.rest[,2:31])
confusionMatrix(data = mod6.out, reference = df.rest$rich)
setwd("~/Documents/PhD/PhD_Core/Teaching/yale_ml_workshop")
library(ggplot2); library(caret); library(dplyr)
df <- read.csv("class_data.csv", as.is=TRUE)
df$X <- NULL
df$rich <- as.factor(df$rich)
head(df[,1:8])
head(df[,1:7])
complete.cases(df) %>% table()
ggplot(data = df) +
geom_boxplot(aes(y = income, x = rich)) + coord_flip()
complete.cases(df) %>% table()
ggplot(data = df) +
geom_boxplot(aes(y = income, x = rich)) + coord_flip()
lr1 <- glm(rich ~ ., family = "binomial", data = df)
summary(lr1)
lr1.out <- fitted(lr1)
ggplot(data = cbind(df, lr1.out)) +
geom_boxplot(aes(y = lr1.out, x = rich), width = 0.2 ) +
coord_flip()
lr1.bin.out <- ifelse(lr1.out < 0.5, "no", "yes")
confusionMatrix(data = lr1.bin.out, reference = df$rich)
df2 <- cbind(df, (df[,2:16] + matrix(nrow = 100, ncol = 15, rnorm(15*100))))
names(df2)[17:31] <- paste0(names(df[2:16]), "_2")
lr2 <- glm(rich ~ ., family = "binomial", data = df2)
View(df2)
df.pc <- prcomp(df2[,-1])$x %>% as.data.frame()
df.pc <- df.pc[,1:11]
df.pc$rich <- df2$rich
pc.LR <- glm(rich ~ ., family = "binomial", data = df.pc)
summary(pc.LR)
pc.LR.out <- pc.LR$fitted.values
pc.LR.out <- ifelse(pc.LR.out < 0.5, "no", "yes")
confusionMatrix(data = pc.LR.out, reference = df.pc$rich)
correlations <- sapply(names(df2)[2:31], function(i) cor(as.numeric(df2[,i]), as.numeric(df2$rich)))
summary(correlations)
fs1 <- names(correlations)[(correlations > 0.1) | (correlations < -1.5)]
df.fs1 <- dplyr::select(df2, one_of(c("rich", fs1)))
fs1.lr <- glm(rich ~ ., family = "binomial", data = df.fs1)
confusionMatrix(data = ifelse(fs1.lr$fitted.values < 0.5, "no", "yes"), reference = df.fs1$rich)
set.seed(1)    # this means that we can get the same random split next time
inSubset <- createDataPartition(df2$rich, p=0.75, list=FALSE)
df.sub   <- df2[inSubset,]
df.rest  <- df2[-inSubset,]
mod.sub <- train(x= as.matrix(df.sub[,2:31]), y = as.factor(df.sub$rich),
method = "svmLinear")
getTrainPerf(mod.sub)
plot(varImp(mod.sub))                             # all of the predictors!
plot(varImp(mod.sub), top = 10)                   # Just top 10, scaled
coef(mod.sub$finalModel) %>% head()
set.seed(1)
mod1 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "lda",
trControl = cvCtrl)
cvCtrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)   # see help for other CV
mod1 <- train(x = as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "lda",
trControl = cvCtrl)
getTrainPerf(mod1)
set.seed(1)
mod2 <- train(x= as.matrix(df.sub[,2:31]),
y = as.factor(df.sub$rich),
method = "svmLinear2",
trControl = cvCtrl)
getTrainPerf(mod2)
mod2.out <- predict(mod2, newdata = df.rest[,2:31])
confusionMatrix(data = mod2.out, reference = df.rest$rich)
